---
title: Perception and adversarial examples
date: 11/4/2021, 3:23:22 PM
---

# Perception and adversarial images

In the human brain, no less than 1/5th of the visual cortex is dedicated to facial recognition. So long as it has improved survival, we've become more and more sensitive to the curves of the human face.

Compare that to how the face of, say, an elephant appears to us, and you and I can simulate how largely our perceptions are distortions of the objective world. In our world, human faces are fundamentally higher-resolution than anything else.

> **The interface theory of perception**: the perceptions of an organism are a user interface between that organism and the objective world

Our perceptions care a little bit about the statistical structure of the universe, but mostly care about our survival. So if you add some noise to a scene which then renders it a physical impossibility, we don't actually care. We don't see it.

An ML model is trained under different conditions.

Could it be that an adversarial image — a picture with added noise, or a single black pixel — is actually a physical impossibility, a perversion of the way EM waves travel in the universe? Could it be that ML models actually learn the underlying statistics of images, and become confused when presented with what seems to them to be an obvious impossibility?

Rather than saying the ML failed because it didn't see the airplane after the noise was added, perhaps the questions is, how come _we_ don't see all the blatant noise that has been added to the airplane?

Marley
11/4/2021, 4:38:00 PM

[Adversarial examples for the OpenAI CLIP in its zero-shot classification regime and their semantic generalization \| Stanislav Fort](https://stanislavfort.github.io/blog/OpenAI_CLIP_adversarial_examples/)

- it genuinely believes that it's seen a frog or a toad.
